{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhbyalOd54ZoemUKSC3DIl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ekissi/Ekissi/blob/main/cnn_dnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bV54oewcoQP3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue May 21 17:54:30 2024\n",
        "\n",
        "@author: ASUS\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import os\n",
        "import re\n",
        "from PIL import Image\n",
        "import h5py\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import glob\n",
        "import fidle\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.layers import  Dense, Dropout, Input, \\\n",
        "     Flatten, GlobalAveragePooling2D\n",
        "\n",
        "def extract_number(filename):\n",
        "    \"\"\"\n",
        "    Extract the number from a filename.\n",
        "    Assumes the filename contains numbers, e.g., 'image123.jpg'.\n",
        "    \"\"\"\n",
        "    match = re.search(r'\\d+', filename)\n",
        "    return int(match.group()) if match else -1\n",
        "\n",
        "def list_files_and_directories_sorted(directory_path):\n",
        "    try:\n",
        "        # Get the list of all files and directories\n",
        "        items = os.listdir(directory_path)\n",
        "        # Sort items by numbers in descending order\n",
        "        items.sort(key=extract_number, reverse=False)\n",
        "        return items\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return []\n",
        "\n",
        "def csv_file(image_dir):\n",
        "    items = list_files_and_directories_sorted(image_dir)\n",
        "    labels = pd.read_excel('MC2_ok.xlsx') # Example labels, should be of the same length as filenames\n",
        "    # Create a list of dictionaries for each image and its label\n",
        "    data = [{'filename': filename, 'label': label} for filename, label in zip(items, labels)]\n",
        "    # Convert the list of dictionaries to a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    # Save the DataFrame to a CSV file\n",
        "    df.to_csv('data.csv', index=False)\n",
        "    return df\n",
        "#data1 = csv_file(image_dir).iloc[range(22)]\n",
        "#csv_file(image_dir)\n",
        "#data = pd.read_csv(image_dir)\n",
        "def split_folder(image_dir,rand_st):\n",
        "    train1_data, test1_data = train_test_split(csv_file(image_dir).iloc[range(22)], test_size=0.2, random_state=rand_st)\n",
        "    train2_data, test2_data = train_test_split(csv_file(image_dir).iloc[range(22,47)], test_size=0.2, random_state=rand_st)\n",
        "    train3_data, test3_data = train_test_split(csv_file(image_dir).iloc[range(47,69)], test_size=0.2, random_state=rand_st)\n",
        "    train4_data, test4_data = train_test_split(csv_file(image_dir).iloc[range(69,85)], test_size=0.2, random_state=rand_st)\n",
        "    train5_data, test5_data = train_test_split(csv_file(image_dir).iloc[range(85,103)], test_size=0.2, random_state=rand_st)\n",
        "    train6_data, test6_data = train_test_split(csv_file(image_dir).iloc[range(103,119)], test_size=0.2, random_state=rand_st)\n",
        "    train7_data, test7_data = train_test_split(csv_file(image_dir).iloc[range(119,133)], test_size=0.2, random_state=rand_st)\n",
        "    train8_data, test8_data = train_test_split(csv_file(image_dir).iloc[range(133,151)], test_size=0.2, random_state=rand_st)\n",
        "    train9_data, test9_data = train_test_split(csv_file(image_dir).iloc[range(151,170)], test_size=0.2, random_state=rand_st)\n",
        "    train10_data, test10_data = train_test_split(csv_file(image_dir).iloc[range(170,209)], test_size=0.2, random_state=rand_st)\n",
        "\n",
        "    train1_data.to_csv('train_data_folder\\\\train1_data.csv', index=False)\n",
        "    train2_data.to_csv('train_data_folder\\\\train2_data.csv', index=False)\n",
        "    train3_data.to_csv('train_data_folder\\\\train3_data.csv', index=False)\n",
        "    train4_data.to_csv('train_data_folder\\\\train4_data.csv', index=False)\n",
        "    train5_data.to_csv('train_data_folder\\\\train5_data.csv', index=False)\n",
        "    train6_data.to_csv('train_data_folder\\\\train6_data.csv', index=False)\n",
        "    train7_data.to_csv('train_data_folder\\\\train7_data.csv', index=False)\n",
        "    train8_data.to_csv('train_data_folder\\\\train8_data.csv', index=False)\n",
        "    train9_data.to_csv('train_data_folder\\\\train9_data.csv', index=False)\n",
        "    train10_data.to_csv('train_data_folder\\\\train10_data.csv', index=False)\n",
        "\n",
        "    test1_data.to_csv('test_data_folder\\\\test1_data.csv', index=False)\n",
        "    test2_data.to_csv('test_data_folder\\\\test2_data.csv', index=False)\n",
        "    test3_data.to_csv('test_data_folder\\\\test3_data.csv', index=False)\n",
        "    test4_data.to_csv('test_data_folder\\\\test4_data.csv', index=False)\n",
        "    test5_data.to_csv('test_data_folder\\\\test5_data.csv', index=False)\n",
        "    test6_data.to_csv('test_data_folder\\\\test6_data.csv', index=False)\n",
        "    test7_data.to_csv('test_data_folder\\\\test7_data.csv', index=False)\n",
        "    test8_data.to_csv('test_data_folder\\\\test8_data.csv', index=False)\n",
        "    test9_data.to_csv('test_data_folder\\\\test9_data.csv', index=False)\n",
        "    test10_data.to_csv('test_data_folder\\\\test10_data.csv', index=False)\n",
        "    return []\n",
        "\n",
        "def associat_csv(chemin_train_csv, chemin_test_csv):\n",
        "    # Lire tous les fichiers CSV dans une liste de DataFrames\n",
        "    fichiers_csv = glob.glob(chemin_train_csv)\n",
        "    liste_dataframes = [pd.read_csv(fichier) for fichier in fichiers_csv]\n",
        "    # Combiner tous les DataFrames en un seul\n",
        "    df_combine = pd.concat(liste_dataframes, ignore_index=True)\n",
        "    # Enregistrer le DataFrame combiné dans un nouveau fichier CSV\n",
        "    df_combine.to_csv('train_data.csv', index=False)\n",
        "\n",
        "    fichiers_csv = glob.glob(chemin_test_csv)\n",
        "    liste_dataframes = [pd.read_csv(fichier) for fichier in fichiers_csv]\n",
        "    # Combiner tous les DataFrames en un seul\n",
        "    df_combine = pd.concat(liste_dataframes, ignore_index=True)\n",
        "    # Enregistrer le DataFrame combiné dans un nouveau fichier CSV\n",
        "    df_combine.to_csv('test_data.csv', index=False)\n",
        "\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    def _init_(self, data, batch_size = 47, image_size=(2448, 2048), n_channels=3,shuffle=False, preprocess=None, list_IDs=[], db_path=''):\n",
        "        'Initialization'\n",
        "        self.preprocess = preprocess\n",
        "        self.data = data\n",
        "        self.list_IDs_temp = []\n",
        "        self.image_size = image_size\n",
        "        self.db_path = db_path\n",
        "        self.batch_size = batch_size\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def _len_(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.data)/self.batch_size))\n",
        "\n",
        "    def _getitem_(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        # Find list of IDs\n",
        "        batch_data = self.data.iloc[indexes]\n",
        "        # Generate data\n",
        "        X,y = self.__data_generation(batch_data)\n",
        "        return X,y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.data))\n",
        "        if self.shuffle == False:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples'  # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.image_size, self.n_channels))\n",
        "        y = np.empty((self.batch_size), dtype=float)\n",
        "        # Generate data\n",
        "        for j, (filename, label) in enumerate(zip(batch_data['filename'], batch_data['label'])):\n",
        "            # img=sklearn.datasets.load_sample_image(self.db_path+ID)\n",
        "            id = os.path.join(self.db_path, filename)\n",
        "            img = image.load_img(id, target_size=self.image_size)\n",
        "            img = image.img_to_array(img)\n",
        "            img = self.preprocess(img)\n",
        "            # x = np.expand_dims(img, axis=0)\n",
        "            X[j, :, :, :] = img\n",
        "            y[j] = label\n",
        "        return X,y\n",
        "\n",
        "# import skimage\n",
        "def Base_Model(base, weights='imagenet', include_top=False, input_shape=(2448, 2048, 3)):\n",
        "    if (base == 'resnet50'):\n",
        "        return ResNet50(weights=weights, include_top=include_top, input_shape=input_shape)\n",
        "    if (base == 'vgg16'):\n",
        "        return VGG16(weights=weights, include_top=include_top, input_shape=input_shape)\n",
        "#base = 'resnet50'\n",
        "# model = Base_Model(base, weights='imagenet', include_top=False, input_shape=(2448, 2048, 3))\n",
        "\n",
        "def Feature_Extraction():\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(2448, 2048, 3))\n",
        "    x = base_model.layers[-1].output\n",
        "    x1 = GlobalAveragePooling2D()(x)\n",
        "    vgg16_model = keras.Model(inputs=base_model.layers[0].output, outputs=x1)\n",
        "    print(\"Model archtecture's details:\")\n",
        "    #vgg16_model.summary()\n",
        "    return vgg16_model\n",
        "\n",
        "def Dense_model(input,dropOutRate=0.25,hidden_units=1024,num_layers=3):\n",
        "       x2=Input(shape=input)\n",
        "       x2_=Flatten()(x2)\n",
        "       for i in range(num_layers):\n",
        "        x2_= Dense(hidden_units,activation='relu',kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),bias_regularizer=l2(0.5))(x2_)\n",
        "        x2_=Dropout(dropOutRate)(x2_)\n",
        "       x3=Dense(1,activation=\"linear\")(x2_)\n",
        "       model=keras.Model(inputs=x2,outputs=x3)\n",
        "       return model\n",
        "\n",
        "#[10,20,30,40,50,60,70,80,90,100,150,200,250,300,350,400,450,500,550,600]\n",
        "random_states =[10,20]\n",
        "mse_test_list = []\n",
        "mse_training_list = []\n",
        "mse_valid_list = []\n",
        "mae_test_list = []\n",
        "mae_training_list = []\n",
        "mae_valid_list = []\n",
        "Rsquared_test_list = []\n",
        "Rsquared_training_list = []\n",
        "for rand_st in random_states:\n",
        "    chemin_test_csv = 'test_data_folder\\\\*.csv'\n",
        "    chemin_train_csv = 'train_data_folder\\\\*.csv'\n",
        "    image_dir = 'imagebatch'\n",
        "    split_folder(image_dir,rand_st)\n",
        "    associat_csv(chemin_train_csv, chemin_test_csv)\n",
        "    train_data = pd.read_csv('train_data.csv')\n",
        "    test_data = pd.read_csv('test_data.csv')\n",
        "    train_generator = DataGenerator(train_data, image_dir)\n",
        "    test_generator = DataGenerator(test_data, image_dir)\n",
        "    base = 'vgg16'\n",
        "    model = Sequential([\n",
        "    Feature_Extraction(),\n",
        "    Flatten(),\n",
        "    Dense_model((512,))\n",
        "    ])\n",
        "    model.summary()\n",
        "\n",
        "    #Add callback\n",
        "    os.makedirs('.\\DEEP_LEARNING', mode=0o750, exist_ok=True)\n",
        "    save_dir = \"best_model.keras\"\n",
        "    savemodel_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_dir, verbose=0,save_best_only=True)\n",
        "    model.compile(optimizer = \"adam\",\n",
        "                          loss      = \"mse\",\n",
        "                          metrics   = [\"mae\", \"mse\"])\n",
        "    history = model.fit(train_generator,\n",
        "                            epochs = 150,\n",
        "                            batch_size = 16,\n",
        "                            validation_data = test_generator,\n",
        "                            callbacks = [savemodel_callback])\n",
        "    # Model evaluation\n",
        "    score = model.evaluate(test_generator, verbose = 0)\n",
        "\n",
        "    print('x_test/loss       : {:5.4f}'.format(score[0]))\n",
        "    print('x_test/mae       : {:5.4f}'.format(score[1]))\n",
        "    print('x_test/mse      : {:5.4f}'.format(score[2]))\n",
        "\n",
        "    print('min(val_mae) : {:5.4f}'.format(min(history.history['val_mae'])))\n",
        "\n",
        "    fidle.scrawler.history(history, plot ={'MSE' : ['mse', 'val_mse'],\n",
        "                                           'MAE' : ['mae', 'val_mae'],\n",
        "                                           'LOSS' :['loss', 'val_loss']}, save_as = '01-history')\n",
        "    mse_tt = float(format(score[2]))\n",
        "    mae_tt = float(format(score[1]))\n",
        "    mse_test_list.append(mse_tt)\n",
        "    mae_test_list.append(mae_tt)\n",
        "    # Restore model\n",
        "    loaded_model = tf.keras.models.load_model('best_model.keras')\n",
        "    loaded_model.summary()\n",
        "    print(\"Loaded.\")\n",
        "\n",
        "    #Evaluate model\n",
        "    score= loaded_model.evaluate(test_generator, verbose = 0)\n",
        "\n",
        "    print('x_test/loss       : {:5.4f}'.format(score[0]))\n",
        "    print('x_test/mae       : {:5.4f}'.format(score[1]))\n",
        "    print('x_test/mse      : {:5.4f}'.format(score[2]))\n",
        "    mse_vd = float(format(score[2]))\n",
        "    mae_vd = float(format(score[1]))\n",
        "    mse_valid_list.append(mse_vd)\n",
        "    mae_valid_list.append(mae_vd)\n",
        "average_mse_tt = np.mean(mse_test_list)\n",
        "standard_deviation_mse_tt = np.std(mse_test_list)\n",
        "\n",
        "average_mae_tt = np.mean(mae_test_list)\n",
        "standard_deviation_mae_tt = np.std(mae_test_list)\n",
        "\n",
        "average_mse_vd = np.mean(mse_valid_list)\n",
        "standard_deviation_mse_vd = np.std(mse_valid_list)\n",
        "\n",
        "average_mae_vd = np.mean(mae_test_list)\n",
        "standard_deviation_mae_vd = np.std(mae_valid_list)\n",
        "\n",
        "print(\"TEST MSE:\",\"               Average:\", average_mse_tt,\"\",\"      SD :\",standard_deviation_mse_tt)\n",
        "print(\"TEST MAE:\",\"               Average:\", average_mae_tt,\"\",\"       SD :\",standard_deviation_mae_tt)\n",
        "\n",
        "print(\"VALIDATION MSE:\",\"         Average:\", average_mse_vd,\"\",\"       SD :\",standard_deviation_mse_vd)\n",
        "print(\"VALIDATION MAE:\",\"         Average:\", average_mae_vd,\"\",\"       SD :\",standard_deviation_mae_vd)\n",
        "\n",
        "#%%\n",
        "#Make prediction\n",
        "n=len(y_test)\n",
        "#◘ii=np.random.randint(1,len(X_test),n)\n",
        "#x_sample = X_test[ii]\n",
        "#y_sample = y_test[ii]\n",
        "\n",
        "y_pred = loaded_model.predict(test_features, verbose = 2)\n",
        "\n",
        "print('MC prediction')\n",
        "for i in range(n):\n",
        "    pred  = y_pred[i][0]\n",
        "    real = y_test[i]\n",
        "    delta = real-pred\n",
        "    print(f' {i:03d}       {pred:.2f}         {real}         {delta:+.2f}')\n",
        "\n",
        "\n",
        "# for m in range(1,6):\n",
        "fd = os.listdir('imagebatch')\n",
        "aa = len(fd)\n",
        "b = aa + 1\n",
        "generator = DataGenerator(os.listdir('imagebatch'),'imagebatch')\n",
        "features = np.empty((aa, 2048))\n",
        "for i in range(1, b):\n",
        "    # img=sklearn.datasets.load_sample_image(self.db_path+ID)\n",
        "    id = os.path.join('imagebatch', 'image' + str(i) + '.jpg')\n",
        "    img = image.load_img(id, target_size=(2448, 2048))\n",
        "    img = image.img_to_array(img)\n",
        "    img = preprocess_input(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    f = model(img)\n",
        "    features[i - 1, :] = f\n",
        "h5f = h5py.File('features_resnet50_ok.h5', 'w')\n",
        "h5f.create_dataset('features', data=features)\n",
        "h5f.close()\n",
        "# del feat_test\n",
        "\n",
        "# features = model.predict(generator)\n",
        "# np.save('features_vgg16.npy', features)\n",
        "\n",
        "# for m in range(1,6):\n",
        "# fd = os.listdir('training' + str(m))\n",
        "# aa=len(fd)\n",
        "#  b =aa+1\n",
        "#  generator=DataGenerator(batch_size=1, dim=(2448,2048), n_channels=3,\n",
        "#     shuffle=False,preprocess=preprocess_input,list_IDs=os.listdir('training' + str(m)),db_path='training' + str(m))\n",
        "#  feat_train = np.empty((aa,2048))\n",
        "# for  i in range(1,b):\n",
        "# img=sklearn.datasets.load_sample_image(self.db_path+ID)\n",
        "# id =os.path.join('training' + str(m), 'image'+str(i)+'.jpg')\n",
        "# img = image.load_img(id, target_size=(2448,2048))\n",
        "#    img = image.img_to_array(img)\n",
        "# img = preprocess_input(img)\n",
        "# img = np.expand_dims(img, axis=0)\n",
        "# f = model(img)\n",
        "# feat_train[i-1,:]=f\n",
        "# h5f = h5py.File('Training_features_resnet50'+str(m)+'.h5', 'w')\n",
        "# h5f.create_dataset('training_features', data=feat_train)\n",
        "# h5f.close()\n",
        "# del feat_train"
      ],
      "metadata": {
        "id": "C5bL0bkAoctn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}